{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import bz2\n",
    "import pickle\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "from pymystem3 import Mystem\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculating TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process(idx, title, text):\n",
    "    if os.path.exists('./data/' + idx + '.text'):\n",
    "        return\n",
    "    \n",
    "    text = ' '.join(lemmatizer.lemmatize(text.lower()))\n",
    "    title = ' '.join(lemmatizer.lemmatize(title.lower()))\n",
    "    \n",
    "    with open('./data/' + idx + '.text', 'wb') as fout:\n",
    "        counts = Counter(pattern.findall(text))\n",
    "        pickle.dump(counts, fout)\n",
    "    with open('./data/' + idx + '.title', 'wb') as fout:\n",
    "        counts = Counter(pattern.findall(title))\n",
    "        pickle.dump(counts, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stemmer = SnowballStemmer('russian', ignore_stopwords=True)\n",
    "cnt = 0\n",
    "n_threads = 32\n",
    "lemmatizer = Mystem()\n",
    "pattern = re.compile('\\d+|[^\\W\\d]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with gzip.open('./docs.tsv.gz', 'rb') as f:\n",
    "    params = [('', '', '') for i in range(n_threads)]\n",
    "    for line in f:\n",
    "        if cnt != 0 and cnt % n_threads == 0:\n",
    "            Parallel(n_jobs=n_threads)(delayed(process)(*params[j]) for j in range(n_threads))\n",
    "\n",
    "        idx, title, text = line.decode('utf-8').split('\\t')\n",
    "        params[cnt % n_threads] = (idx, title, text)\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#count files length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pattern = re.compile('\\d+|[^\\W\\d]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title_lengths = dict()\n",
    "text_lengths = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('lengths.title', 'r') as fin:\n",
    "    for line in fin:\n",
    "        spl = line.strip().split(' ')\n",
    "        title_lengths[spl[0]] = int(spl[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.3285758210273"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(title_lengths.values()) / len(title_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('lengths.text', 'r') as fin:\n",
    "    for line in fin:\n",
    "        spl = line.strip().split(' ')\n",
    "        text_lengths[spl[0]] = int(spl[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9717.212767126959"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(text_lengths.values()) / len(title_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./lengths.title', 'wb') as fout:\n",
    "    pickle.dump(title_lengths, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./lengths.text', 'wb') as fout:\n",
    "    pickle.dump(text_lengths, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#removing numbers of 5+ digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = os.listdir('./data')\n",
    "n_threads = 8\n",
    "per = len(files) // n_threads + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dpattern = re.compile('\\d{5}\\d*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process1(idx, fnames):\n",
    "    for fname, cnt in zip(fnames, range(len(fnames))):\n",
    "        if (cnt + 1) % 15000 == 0:\n",
    "            print(idx, 'finished', cnt)\n",
    "        dc = {}\n",
    "        with open('./data/' + fname, 'rb') as fin:\n",
    "            dc = pickle.load(fin)\n",
    "            \n",
    "        ndc = dict(filter(lambda x: not dpattern.match(x[0]), dc.items()))\n",
    "        with open('./data/' + fname, 'wb') as fout:\n",
    "            pickle.dump(ndc, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Parallel(n_jobs=n_threads)(delayed(process1)(j, files[j*per : (j+1)*per]) for j in range(n_threads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#stemming and filter stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('russian', ignore_stopwords=True)\n",
    "stop = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process2(idx, fnames):\n",
    "    for fname, cnt in zip(fnames, range(len(fnames))):\n",
    "        if (cnt + 1) % 15000 == 0:\n",
    "            print(idx, 'finished', cnt)\n",
    "        dc = {}\n",
    "        with open('./data/' + fname, 'rb') as fin:\n",
    "            dc = pickle.load(fin)\n",
    "\n",
    "        ndc = filter(lambda x: not (x[0] in stop), dc.items())\n",
    "        ndc = dict(map(lambda x: (stemmer.stem(x[0]), x[1]), ndc))\n",
    "        with open('./data/' + fname, 'wb') as fout:\n",
    "            pickle.dump(ndc, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Parallel(n_jobs=n_threads)(delayed(process2)(j, files[j*per : (j+1)*per]) for j in range(n_threads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! echo 'hello' > log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fix queries layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_eng_chars = u\"~!@#$%^&qwertyuiop[]asdfghjkl;'zxcvbnm,./QWERTYUIOP{}ASDFGHJKL:\\\"|ZXCVBNM<>?\"\n",
    "_rus_chars = u\"ё!\\\"№;%:?йцукенгшщзхъфывапролджэячсмитьбю.ЙЦУКЕНГШЩЗХЪФЫВАПРОЛДЖЭ/ЯЧСМИТЬБЮ,\"\n",
    "_trans_table = dict(zip(_eng_chars, _rus_chars))\n",
    " \n",
    "def fix_layout(s):\n",
    "    return u''.join([_trans_table.get(c, c) for c in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60000/60000 [00:25<00:00, 2399.78it/s]\n"
     ]
    }
   ],
   "source": [
    "#partial words set\n",
    "ds = set()\n",
    "for f in tqdm(os.listdir('./data')[0:60000]):\n",
    "    with open('./data/' + f, 'rb') as fin:\n",
    "        ds.update(pickle.load(fin).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enw = re.compile('^[a-zA-Z]+$')\n",
    "def fix(s):\n",
    "    if not enw.match(s):\n",
    "        return s\n",
    "    fx = stemmer.stem(lemmatizer.lemmatize(fix_layout(s))[0])\n",
    "    \n",
    "    if (fx in ds) or fix_layout(s) in stop:\n",
    "        return fix_layout(s)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fout = open('./queries.fix', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./queries.tsv', 'r') as fin:\n",
    "    for line in fin:\n",
    "        idx, line = line.split('\\t')\n",
    "        words = pattern.findall(line)\n",
    "        res = ' '.join([fix(word) for word in words])\n",
    "        fout.write(idx + '\\t' + res + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create total words dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processd(idx, fnames):\n",
    "    cnt = 0\n",
    "    df = Counter()\n",
    "    buf = Counter()\n",
    "    for fname, cnt in zip(fnames, range(len(fnames))):\n",
    "        if (cnt + 1) % 1000 == 0:\n",
    "            df += buf\n",
    "            buf.clear()\n",
    "            \n",
    "        if (cnt + 1) % 5000 == 0:\n",
    "            print(idx, 'finished', cnt)\n",
    "        \n",
    "        pr = Counter()\n",
    "        with open('./data/' + fname, 'rb') as fin:\n",
    "            pr = Counter(pickle.load(fin))\n",
    "        buf += pr\n",
    "    df += buf\n",
    "    with open(str(idx) + '.dict', 'wb') as fout:\n",
    "        pickle.dump(df, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = os.listdir('./data')\n",
    "n_threads = 30\n",
    "per = len(files) // n_threads + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Parallel(n_jobs=n_threads)(delayed(processd)(j, files[j*per : (j+1)*per]) for j in range(n_threads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = Counter()\n",
    "for i in range(30):\n",
    "    with open(str(i) + '.dict', 'rb') as fin:\n",
    "        df += pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.pop('')\n",
    "with open('./total.dict', 'wb') as fout:\n",
    "    pickle.dump(df, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf and df for query words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qw = set()\n",
    "with open('queries.final', 'r') as fin:\n",
    "    for line in fin:\n",
    "        idx, line = line.split('\\t')\n",
    "        words = pattern.findall(line)\n",
    "        for word in words:\n",
    "            w = stemmer.stem(lemmatizer.lemmatize(word)[0])\n",
    "            qw.add(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8922"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "totalf = dict()\n",
    "with open('total.dict', 'rb') as fin:\n",
    "    totalf = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qw2 = set()\n",
    "for w in qw:\n",
    "    if not (w in totalf):\n",
    "        continue\n",
    "    qw2.add(w)\n",
    "qw = qw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8895"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titlefiles = !ls ./data | grep '\\.title'\n",
    "textfiles = !ls ./data | grep '\\.text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processF(idx, fnames):\n",
    "    for fname, cnt in zip(fnames, range(len(fnames))):\n",
    "        if (cnt + 1) % 10000 == 0:\n",
    "            print(idx, 'finished', cnt)\n",
    "        \n",
    "        if os.path.exists('./frequences/' + fname):\n",
    "            continue\n",
    "        \n",
    "        dc = {}\n",
    "        with open('./data/' + fname, 'rb') as fin:\n",
    "            dc = pickle.load(fin)\n",
    "        dc = dict(list(filter(lambda x: x[0] in qw, dc.items())))\n",
    "        with open('./frequences/' + fname, 'wb') as fout:\n",
    "            pickle.dump(dc, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_threads = 8\n",
    "per = len(titlefiles) // n_threads + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Parallel(n_jobs=n_threads)(delayed(processF)(j, titlefiles[j*per : (j+1)*per]) for j in range(n_threads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 finished 9999\n",
      "6 finished 9999\n",
      "2 finished 9999\n",
      "0 finished 9999\n",
      "7 finished 9999\n",
      "5 finished 9999\n",
      "1 finished 9999\n",
      "4 finished 9999\n",
      "6 finished 19999\n",
      "7 finished 19999\n",
      "2 finished 19999\n",
      "0 finished 19999\n",
      "3 finished 19999\n",
      "1 finished 19999\n",
      "5 finished 19999\n",
      "4 finished 19999\n",
      "6 finished 29999\n",
      "7 finished 29999\n",
      "2 finished 29999\n",
      "0 finished 29999\n",
      "1 finished 29999\n",
      "3 finished 29999\n",
      "5 finished 29999\n",
      "7 finished 39999\n",
      "4 finished 29999\n",
      "6 finished 39999\n",
      "2 finished 39999\n",
      "0 finished 39999\n",
      "1 finished 39999\n",
      "5 finished 39999\n",
      "3 finished 39999\n",
      "6 finished 49999\n",
      "7 finished 49999\n",
      "4 finished 39999\n",
      "2 finished 49999\n",
      "3 finished 49999\n",
      "0 finished 49999\n",
      "5 finished 49999\n",
      "1 finished 49999\n",
      "6 finished 59999\n",
      "7 finished 59999\n",
      "2 finished 59999\n",
      "4 finished 49999\n",
      "3 finished 59999\n",
      "0 finished 59999\n",
      "5 finished 59999\n",
      "1 finished 59999\n",
      "2 finished 69999\n",
      "6 finished 69999\n",
      "7 finished 69999\n",
      "4 finished 59999\n",
      "0 finished 69999\n",
      "3 finished 69999\n",
      "5 finished 69999\n",
      "1 finished 69999\n",
      "4 finished 69999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Parallel(n_jobs=n_threads)(delayed(processF)(j, textfiles[j*per : (j+1)*per]) for j in range(n_threads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title_df = Counter()\n",
    "text_df = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processDF(idx, fnames):\n",
    "    df = Counter()\n",
    "    for fname, cnt in zip(fnames, range(len(fnames))):\n",
    "        if (cnt + 1) % 10000 == 0:\n",
    "            print(idx, 'finished', cnt+1)\n",
    "            \n",
    "        with open('./frequences/' + fname, 'rb') as fin:\n",
    "            df.update(pickle.load(fin).keys())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_threads = 8\n",
    "per = len(textfiles) // n_threads + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 finished 10000\n",
      "0 finished 10000\n",
      "2 finished 10000\n",
      "3 finished 10000\n",
      "4 finished 10000\n",
      "7 finished 10000\n",
      "6 finished 10000\n",
      "5 finished 10000\n",
      "2 finished 20000\n",
      "1 finished 20000\n",
      "0 finished 20000\n",
      "3 finished 20000\n",
      "7 finished 20000\n",
      "4 finished 20000\n",
      "6 finished 20000\n",
      "5 finished 20000\n",
      "2 finished 30000\n",
      "1 finished 30000\n",
      "0 finished 30000\n",
      "3 finished 30000\n",
      "7 finished 30000\n",
      "4 finished 30000\n",
      "6 finished 30000\n",
      "5 finished 30000\n",
      "2 finished 40000\n",
      "1 finished 40000\n",
      "3 finished 40000\n",
      "0 finished 40000\n",
      "7 finished 40000\n",
      "4 finished 40000\n",
      "6 finished 40000\n",
      "5 finished 40000\n",
      "2 finished 50000\n",
      "1 finished 50000\n",
      "3 finished 50000\n",
      "0 finished 50000\n",
      "7 finished 50000\n",
      "4 finished 50000\n",
      "6 finished 50000\n",
      "5 finished 50000\n",
      "2 finished 60000\n",
      "1 finished 60000\n",
      "3 finished 60000\n",
      "0 finished 60000\n",
      "7 finished 60000\n",
      "4 finished 60000\n",
      "6 finished 60000\n",
      "5 finished 60000\n",
      "1 finished 70000\n",
      "2 finished 70000\n",
      "0 finished 70000\n",
      "3 finished 70000\n",
      "7 finished 70000\n",
      "5 finished 70000\n",
      "4 finished 70000\n",
      "6 finished 70000\n"
     ]
    }
   ],
   "source": [
    "title_dfs = Parallel(n_jobs=n_threads)(delayed(processDF)(j, titlefiles[j*per : (j+1)*per]) for j in range(n_threads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for df in title_dfs:\n",
    "    title_df += df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./title.df', 'wb') as fout:\n",
    "    pickle.dump(title_df, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 finished 10000\n",
      "6 finished 10000\n",
      "0 finished 10000\n",
      "2 finished 20000\n",
      "6 finished 20000\n",
      "4 finished 10000\n",
      "0 finished 20000\n",
      "3 finished 10000\n",
      "7 finished 10000\n",
      "5 finished 10000\n",
      "1 finished 10000\n",
      "4 finished 20000\n",
      "7 finished 20000\n",
      "3 finished 20000\n",
      "1 finished 20000\n",
      "5 finished 20000\n",
      "6 finished 30000\n",
      "2 finished 30000\n",
      "0 finished 30000\n",
      "7 finished 30000\n",
      "1 finished 30000\n",
      "3 finished 30000\n",
      "5 finished 30000\n",
      "7 finished 40000\n",
      "4 finished 30000\n",
      "6 finished 40000\n",
      "2 finished 40000\n",
      "0 finished 40000\n",
      "1 finished 40000\n",
      "5 finished 40000\n",
      "3 finished 40000\n",
      "6 finished 50000\n",
      "7 finished 50000\n",
      "4 finished 40000\n",
      "2 finished 50000\n",
      "3 finished 50000\n",
      "0 finished 50000\n",
      "5 finished 50000\n",
      "1 finished 50000\n",
      "6 finished 60000\n",
      "7 finished 60000\n",
      "2 finished 60000\n",
      "4 finished 50000\n",
      "3 finished 60000\n",
      "0 finished 60000\n",
      "5 finished 60000\n",
      "1 finished 60000\n",
      "2 finished 70000\n",
      "6 finished 70000\n",
      "7 finished 70000\n",
      "4 finished 60000\n",
      "0 finished 70000\n",
      "3 finished 70000\n",
      "5 finished 70000\n",
      "1 finished 70000\n",
      "4 finished 70000\n"
     ]
    }
   ],
   "source": [
    "text_dfs = Parallel(n_jobs=n_threads)(delayed(processDF)(j, textfiles[j*per : (j+1)*per]) for j in range(n_threads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for df in text_dfs:\n",
    "    text_df += df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./text.df', 'wb') as fout:\n",
    "    pickle.dump(text_df, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#total document frequency (no dependency on zone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processTDF(idx, fnames):\n",
    "    df = Counter()\n",
    "    for fname, cnt in zip(fnames, range(len(fnames))):\n",
    "        if (cnt + 1) % 10000 == 0:\n",
    "            print(idx, 'finished', cnt+1)\n",
    "            \n",
    "        doc_idx = fname.split('.')[0]\n",
    "        \n",
    "        ks1 = []\n",
    "        ks2 = []\n",
    "        with open('./frequences/' + doc_idx + '.title', 'rb') as fin:\n",
    "            ks1 = pickle.load(fin).keys()\n",
    "        with open('./frequences/' + doc_idx + '.text', 'rb') as fin:\n",
    "            ks2 = pickle.load(fin).keys()\n",
    "\n",
    "        df.update(set(ks1) | set(ks2))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_threads = 8\n",
    "per = len(textfiles) // n_threads + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 finished 10000\n",
      "6 finished 10000\n",
      "2 finished 10000\n",
      "0 finished 10000\n",
      "7 finished 10000\n",
      "5 finished 10000\n",
      "1 finished 10000\n",
      "4 finished 10000\n",
      "6 finished 20000\n",
      "7 finished 20000\n",
      "2 finished 20000\n",
      "0 finished 20000\n",
      "3 finished 20000\n",
      "1 finished 20000\n",
      "5 finished 20000\n",
      "4 finished 20000\n",
      "6 finished 30000\n",
      "7 finished 30000\n",
      "2 finished 30000\n",
      "0 finished 30000\n",
      "1 finished 30000\n",
      "3 finished 30000\n",
      "5 finished 30000\n",
      "4 finished 30000\n",
      "7 finished 40000\n",
      "6 finished 40000\n",
      "2 finished 40000\n",
      "0 finished 40000\n",
      "1 finished 40000\n",
      "5 finished 40000\n",
      "3 finished 40000\n",
      "6 finished 50000\n",
      "7 finished 50000\n",
      "4 finished 40000\n",
      "2 finished 50000\n",
      "3 finished 50000\n",
      "0 finished 50000\n",
      "5 finished 50000\n",
      "1 finished 50000\n",
      "6 finished 60000\n",
      "7 finished 60000\n",
      "2 finished 60000\n",
      "4 finished 50000\n",
      "3 finished 60000\n",
      "0 finished 60000\n",
      "5 finished 60000\n",
      "1 finished 60000\n",
      "2 finished 70000\n",
      "6 finished 70000\n",
      "7 finished 70000\n",
      "4 finished 60000\n",
      "0 finished 70000\n",
      "3 finished 70000\n",
      "5 finished 70000\n",
      "1 finished 70000\n",
      "4 finished 70000\n"
     ]
    }
   ],
   "source": [
    "total_dfs = Parallel(n_jobs=n_threads)(delayed(processTDF)(j, textfiles[j*per : (j+1)*per]) for j in range(n_threads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_df = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for df in total_dfs:\n",
    "    total_df += df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./total.df', 'wb') as fout:\n",
    "    pickle.dump(total_df, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8895"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create 3-gramms dictionary for DSSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pattern = re.compile('\\d+|[^\\W\\d]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d3 = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 20000\n",
      "processed 40000\n",
      "processed 60000\n",
      "processed 80000\n",
      "processed 100000\n",
      "processed 120000\n",
      "processed 140000\n",
      "processed 160000\n",
      "processed 180000\n",
      "processed 200000\n",
      "processed 220000\n",
      "processed 240000\n",
      "processed 260000\n",
      "processed 280000\n",
      "processed 300000\n",
      "processed 320000\n",
      "processed 340000\n",
      "processed 360000\n",
      "processed 380000\n",
      "processed 400000\n",
      "processed 420000\n",
      "processed 440000\n",
      "processed 460000\n",
      "processed 480000\n",
      "processed 500000\n",
      "processed 520000\n",
      "processed 540000\n",
      "processed 560000\n",
      "processed 580000\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "with open('titles.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        cnt += 1\n",
    "        \n",
    "        if cnt % 20000 == 0:\n",
    "            print('processed', cnt)\n",
    "        \n",
    "        title = line.split('\\t')[1].lower()\n",
    "        title = '#' + '#'.join(pattern.findall(title)) + '#'\n",
    "        gg = filter(lambda x: x[1] != '#', ngrams(list(title), 3))\n",
    "        grams3 = map(lambda x: ''.join(x), gg)\n",
    "        d3.update(grams3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42365"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ть#', 186925),\n",
       " ('#по', 168149),\n",
       " ('#пр', 152950),\n",
       " ('#на', 139423),\n",
       " ('#ка', 138145),\n",
       " ('на#', 130092),\n",
       " ('ия#', 126663),\n",
       " ('#в#', 125282),\n",
       " ('ие#', 118564),\n",
       " ('ени', 118020)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d3.most_common()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x: ' ' in x , list(d3.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('3gramms.dict', 'wb') as fout:\n",
    "    pickle.dump(d3, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process3G(idx, files):\n",
    "    d3g = Counter()\n",
    "    for fname, cnt in zip(files, range(len(files))):\n",
    "        with bz2.BZ2File('./clicks/2017/' + fname, 'r') as fin:\n",
    "            for line in fin:\n",
    "                text = line.decode('utf-8').split('@')[0].lower()\n",
    "                text = '#' + '#'.join(pattern.findall(text)) + '#'\n",
    "                gg = filter(lambda x: x[1] != '#', ngrams(list(text), 3))\n",
    "                grams3 = map(lambda x: ''.join(x), gg)\n",
    "                d3g.update(grams3)\n",
    "        if (cnt + 1) % 10 == 0:\n",
    "            print(idx, 'processed', cnt)\n",
    "            \n",
    "    return d3g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = os.listdir('clicks/2017/')\n",
    "n_threads = 32\n",
    "per = len(files) // n_threads + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 processed 9\n",
      "13 processed 9\n",
      "5 processed 9\n",
      "21 processed 9\n",
      "24 processed 9\n",
      "18 processed 9\n",
      "8 processed 9\n",
      "27 processed 9\n",
      "11 processed 9\n",
      "19 processed 9\n",
      "16 processed 9\n",
      "10 processed 9\n",
      "15 processed 9\n",
      "23 processed 9\n",
      "22 processed 9\n",
      "2 processed 9\n",
      "9 processed 9\n",
      "20 processed 9\n",
      "0 processed 9\n",
      "30 processed 9\n",
      "3 processed 9\n",
      "25 processed 9\n",
      "7 processed 9\n",
      "14 processed 9\n",
      "26 processed 19\n",
      "29 processed 9\n",
      "1 processed 9\n",
      "28 processed 9\n",
      "10 processed 19\n",
      "6 processed 9\n",
      "17 processed 9\n",
      "31 processed 9\n",
      "24 processed 19\n",
      "12 processed 9\n",
      "4 processed 9\n",
      "13 processed 19\n",
      "8 processed 19\n",
      "23 processed 19\n",
      "21 processed 19\n",
      "16 processed 19\n",
      "27 processed 19\n",
      "5 processed 19\n",
      "15 processed 19\n",
      "3 processed 19\n",
      "7 processed 19\n",
      "13 processed 29\n",
      "20 processed 19\n",
      "11 processed 19\n",
      "14 processed 19\n",
      "19 processed 19\n",
      "2 processed 19\n",
      "0 processed 19\n",
      "30 processed 19\n",
      "22 processed 19\n",
      "25 processed 19\n",
      "17 processed 19\n",
      "29 processed 19\n",
      "23 processed 29\n",
      "24 processed 29\n",
      "1 processed 19\n",
      "18 processed 19\n",
      "28 processed 19\n",
      "6 processed 19\n",
      "4 processed 19\n",
      "21 processed 29\n",
      "15 processed 29\n",
      "26 processed 29\n",
      "16 processed 29\n",
      "5 processed 29\n",
      "19 processed 29\n",
      "14 processed 29\n",
      "10 processed 29\n",
      "9 processed 19\n",
      "31 processed 19\n",
      "8 processed 29\n",
      "7 processed 29\n",
      "27 processed 29\n",
      "12 processed 19\n",
      "17 processed 29\n",
      "18 processed 29\n",
      "20 processed 29\n",
      "13 processed 39\n",
      "21 processed 39\n",
      "25 processed 29\n",
      "28 processed 29\n",
      "15 processed 39\n",
      "12 processed 29\n",
      "22 processed 29\n",
      "2 processed 29\n",
      "1 processed 29\n",
      "16 processed 39\n",
      "11 processed 29\n",
      "30 processed 29\n",
      "17 processed 39\n",
      "0 processed 29\n",
      "26 processed 39\n",
      "19 processed 39\n",
      "10 processed 39\n",
      "6 processed 29\n",
      "24 processed 39\n",
      "5 processed 39\n",
      "7 processed 39\n",
      "29 processed 29\n",
      "31 processed 29\n",
      "27 processed 39\n",
      "4 processed 29\n",
      "20 processed 39\n",
      "28 processed 39\n",
      "13 processed 49\n",
      "3 processed 29\n",
      "9 processed 29\n",
      "8 processed 39\n",
      "12 processed 39\n",
      "24 processed 49\n",
      "14 processed 39\n",
      "16 processed 49\n",
      "23 processed 39\n",
      "15 processed 49\n",
      "17 processed 49\n",
      "4 processed 39\n",
      "3 processed 39\n",
      "2 processed 39\n",
      "7 processed 49\n",
      "22 processed 39\n",
      "25 processed 39\n",
      "21 processed 49\n",
      "18 processed 39\n",
      "30 processed 39\n",
      "11 processed 39\n",
      "27 processed 49\n",
      "12 processed 49\n",
      "9 processed 39\n",
      "29 processed 39\n",
      "19 processed 49\n",
      "7 processed 59\n",
      "13 processed 59\n",
      "24 processed 59\n",
      "16 processed 59\n",
      "28 processed 49\n",
      "5 processed 49\n",
      "20 processed 49\n",
      "31 processed 39\n",
      "0 processed 39\n",
      "21 processed 59\n",
      "10 processed 49\n",
      "1 processed 39\n",
      "15 processed 59\n",
      "25 processed 49\n",
      "6 processed 39\n",
      "4 processed 49\n",
      "26 processed 49\n",
      "17 processed 59\n",
      "8 processed 49\n",
      "18 processed 49\n",
      "12 processed 59\n",
      "23 processed 49\n",
      "9 processed 49\n",
      "30 processed 49\n",
      "2 processed 49\n",
      "14 processed 49\n",
      "3 processed 49\n",
      "5 processed 59\n",
      "6 processed 49\n",
      "29 processed 49\n",
      "20 processed 59\n",
      "22 processed 49\n",
      "27 processed 59\n",
      "25 processed 59\n",
      "19 processed 59\n",
      "26 processed 59\n",
      "0 processed 49\n",
      "18 processed 59\n",
      "28 processed 59\n",
      "4 processed 59\n",
      "23 processed 59\n",
      "9 processed 59\n",
      "2 processed 59\n",
      "3 processed 59\n",
      "8 processed 59\n",
      "1 processed 49\n",
      "11 processed 49\n",
      "10 processed 59\n",
      "30 processed 59\n",
      "6 processed 59\n",
      "29 processed 59\n",
      "22 processed 59\n",
      "0 processed 59\n",
      "14 processed 59\n",
      "11 processed 59\n",
      "1 processed 59\n"
     ]
    }
   ],
   "source": [
    "d3gs = Parallel(n_jobs=n_threads)(delayed(process3G)(j, files[j*per : (j+1)*per]) for j in range(n_threads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d3g = Counter()\n",
    "for d in d3gs:\n",
    "    d3g += d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('3gramms.dict', 'rb') as fin:\n",
    "    d3g += pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162791"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d3g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('3gramms-total.dict', 'wb') as fout:\n",
    "    pickle.dump(d3g, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ыщё', 10)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d3g.most_common()[85000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d3g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d3g = {}\n",
    "with open('3gramms-total.dict', 'rb') as fin:\n",
    "    d3g = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare dataset for dssm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fixurl(url):\n",
    "    res = url\n",
    "    if res.startswith('http://'):\n",
    "        res = res[7:]\n",
    "        \n",
    "    if res.startswith('https://'):\n",
    "        res = res[8:]\n",
    "        \n",
    "    if res.startswith('www.'):\n",
    "        res = res[4:]\n",
    "        \n",
    "    if res.endswith('/'):\n",
    "        res = res[:-1]\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url2id = dict()\n",
    "with open('./url.data', 'r') as fin:\n",
    "    for line in fin:\n",
    "        idx, url = line.strip().split('\\t')\n",
    "    \n",
    "# with these fixes len(url2id) = 582092 < 582167\n",
    "        url = fixurl(url)\n",
    "        \n",
    "#         if url in url2id:\n",
    "#             print(url)\n",
    "    \n",
    "        url2id[url] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "582094"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(url2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id2title = dict()\n",
    "with open('./titles.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "#         print(line)\n",
    "        splits = line.strip().lower().split('\\t')\n",
    "        if len(splits) == 1:\n",
    "            id2title[splits[0]] = ''\n",
    "        else:\n",
    "            id2title[splits[0]] = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "582167"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id2title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processCL(idx, files):\n",
    "    samples = []\n",
    "    cur_showed = set()\n",
    "    cur_clicked = set()\n",
    "    cur_q = ''\n",
    "\n",
    "    for fname, cnt in zip(files, range(len(files))):\n",
    "        with bz2.BZ2File('./clicks/2017/' + fname) as fin:\n",
    "            for line in fin:\n",
    "                line = line.decode('utf-8').strip()\n",
    "                q, showed, clicked = line.split('\\t')[:3]\n",
    "                q = q.split('@')[0]\n",
    "            \n",
    "                if q != cur_q:\n",
    "                    if cur_q != '' and len(cur_clicked) != 0:\n",
    "                        samples.append('\\t'.join([cur_q, ','.join(cur_showed), ','.join(cur_clicked)]))\n",
    "                    cur_showed.clear()\n",
    "                    cur_clicked.clear()\n",
    "                    cur_q = q\n",
    "            \n",
    "                showed = map(lambda x: url2id[x], filter(lambda x: x in url2id, map(fixurl, showed.split(','))))\n",
    "                clicked = map(lambda x: url2id[x], filter(lambda x: x in url2id, map(fixurl, clicked.split(','))))\n",
    "            \n",
    "                cur_showed.update(showed)\n",
    "                cur_clicked.update(clicked)\n",
    "        \n",
    "            if cur_q != '' and len(cur_clicked) != 0:\n",
    "                samples.append('\\t'.join([cur_q, ','.join(cur_showed), ','.join(cur_clicked)]))\n",
    "            cur_showed.clear()\n",
    "            cur_clicked.clear()\n",
    "            cur_q = ''\n",
    "        \n",
    "        if (cnt + 1) % 10 == 0:\n",
    "            print(idx, 'processed', cnt+1)\n",
    "            \n",
    "            with open('./clicks/filtered/' + str(idx) + '_' + str(cnt) + '.txt', 'w') as fout:\n",
    "                fout.write('\\n'.join(samples))\n",
    "                samples.clear()\n",
    "                \n",
    "    if len(samples) != 0:\n",
    "        with open('./clicks/filtered/' + str(idx) + '_' + str(cnt) + '.txt', 'w') as fout:\n",
    "            fout.write('\\n'.join(samples))\n",
    "            samples.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = os.listdir('./clicks/2017/')\n",
    "n_threads = 32\n",
    "per = len(files) // n_threads + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 processed 10\n",
      "26 processed 10\n",
      "5 processed 10\n",
      "24 processed 10\n",
      "21 processed 10\n",
      "27 processed 10\n",
      "8 processed 10\n",
      "11 processed 10\n",
      "16 processed 10\n",
      "18 processed 10\n",
      "15 processed 10\n",
      "10 processed 10\n",
      "19 processed 10\n",
      "23 processed 10\n",
      "20 processed 10\n",
      "2 processed 10\n",
      "9 processed 10\n",
      "22 processed 10\n",
      "0 processed 10\n",
      "25 processed 10\n",
      "7 processed 10\n",
      "3 processed 10\n",
      "30 processed 10\n",
      "14 processed 10\n",
      "31 processed 10\n",
      "1 processed 10\n",
      "17 processed 10\n",
      "6 processed 10\n",
      "4 processed 10\n",
      "28 processed 10\n",
      "29 processed 10\n",
      "26 processed 20\n",
      "10 processed 20\n",
      "24 processed 20\n",
      "13 processed 20\n",
      "23 processed 20\n",
      "8 processed 20\n",
      "16 processed 20\n",
      "21 processed 20\n",
      "12 processed 10\n",
      "27 processed 20\n",
      "15 processed 20\n",
      "5 processed 20\n",
      "20 processed 20\n",
      "3 processed 20\n",
      "11 processed 20\n",
      "7 processed 20\n",
      "13 processed 30\n",
      "14 processed 20\n",
      "0 processed 20\n",
      "2 processed 20\n",
      "19 processed 20\n",
      "17 processed 20\n",
      "25 processed 20\n",
      "30 processed 20\n",
      "22 processed 20\n",
      "29 processed 20\n",
      "1 processed 20\n",
      "24 processed 30\n",
      "4 processed 20\n",
      "28 processed 20\n",
      "18 processed 20\n",
      "6 processed 20\n",
      "31 processed 20\n",
      "23 processed 30\n",
      "15 processed 30\n",
      "16 processed 30\n",
      "26 processed 30\n",
      "21 processed 30\n",
      "5 processed 30\n",
      "14 processed 30\n",
      "10 processed 30\n",
      "27 processed 30\n",
      "17 processed 30\n",
      "7 processed 30\n",
      "19 processed 30\n",
      "20 processed 30\n",
      "18 processed 30\n",
      "9 processed 20\n",
      "8 processed 30\n",
      "12 processed 20\n",
      "13 processed 40\n",
      "25 processed 30\n",
      "0 processed 30\n",
      "28 processed 30\n",
      "15 processed 40\n",
      "11 processed 30\n",
      "1 processed 30\n",
      "2 processed 30\n",
      "21 processed 40\n",
      "17 processed 40\n",
      "16 processed 40\n",
      "31 processed 30\n",
      "30 processed 30\n",
      "22 processed 30\n",
      "12 processed 30\n",
      "26 processed 40\n",
      "6 processed 30\n",
      "24 processed 40\n",
      "19 processed 40\n",
      "10 processed 40\n",
      "4 processed 30\n",
      "5 processed 40\n",
      "7 processed 40\n",
      "27 processed 40\n",
      "20 processed 40\n",
      "29 processed 30\n",
      "13 processed 50\n",
      "28 processed 40\n",
      "24 processed 50\n",
      "3 processed 30\n",
      "14 processed 40\n",
      "8 processed 40\n",
      "9 processed 30\n",
      "15 processed 50\n",
      "4 processed 40\n",
      "23 processed 40\n",
      "12 processed 40\n",
      "17 processed 50\n",
      "16 processed 50\n",
      "2 processed 40\n",
      "25 processed 40\n",
      "18 processed 40\n",
      "7 processed 50\n",
      "3 processed 40\n",
      "22 processed 40\n",
      "30 processed 40\n",
      "27 processed 50\n",
      "31 processed 40\n",
      "21 processed 50\n",
      "11 processed 40\n",
      "24 processed 60\n",
      "29 processed 40\n",
      "20 processed 50\n",
      "13 processed 60\n",
      "19 processed 50\n",
      "9 processed 40\n",
      "12 processed 50\n",
      "7 processed 60\n",
      "5 processed 50\n",
      "28 processed 50\n",
      "1 processed 40\n",
      "0 processed 40\n",
      "16 processed 60\n",
      "10 processed 50\n",
      "15 processed 60\n",
      "4 processed 50\n",
      "6 processed 40\n",
      "25 processed 50\n",
      "21 processed 60\n",
      "23 processed 50\n",
      "26 processed 50\n",
      "17 processed 60\n",
      "30 processed 50\n",
      "18 processed 50\n",
      "2 processed 50\n",
      "8 processed 50\n",
      "9 processed 50\n",
      "6 processed 50\n",
      "3 processed 50\n",
      "14 processed 50\n",
      "27 processed 60\n",
      "12 processed 60\n",
      "20 processed 60\n",
      "22 processed 50\n",
      "5 processed 60\n",
      "0 processed 50\n",
      "4 processed 60\n",
      "29 processed 50\n",
      "25 processed 60\n",
      "19 processed 60\n",
      "1 processed 50\n",
      "2 processed 60\n",
      "23 processed 60\n",
      "28 processed 60\n",
      "26 processed 60\n",
      "3 processed 60\n",
      "18 processed 60\n",
      "9 processed 60\n",
      "8 processed 60\n",
      "6 processed 60\n",
      "11 processed 50\n",
      "10 processed 60\n",
      "30 processed 60\n",
      "22 processed 60\n",
      "0 processed 60\n",
      "29 processed 60\n",
      "14 processed 60\n",
      "1 processed 60\n",
      "11 processed 60\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Parallel(n_jobs=n_threads)(delayed(processCL)(j, files[j*per : (j+1)*per]) for j in range(n_threads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5722"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arr = np.array([10, 11, 32, 14, 12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 12, 32, 11])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.permutation(arr)[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
